{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Ic1xc9rimg"
      },
      "source": [
        "Implement a transformer architecture and improve and analyze it's arous aspects rather than focus on a particular paper\n",
        "\n",
        "Use percentage changes as data source instead of prices\n",
        "\n",
        "If we make a predictor which just guesses next day's price as the same as todayâ€™s price, it would have better than 95% accuracy. \n",
        "\n",
        "Guessing whether next day price will go up or down i.e. as a classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kEwuEgTyI7Bf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/MyDrive/CS7643 Final Project/'  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data.get_dataset import GetDataset, StockData\n",
        "\n",
        "FAKE_DATA=False\n",
        "REPEAT_ONE_SMALL_BATCH=False\n",
        "NUM_DAYS = 5\n",
        "USE_TRANSFORMER=True\n",
        "USE_ALT_TRANSFORMER = True\n",
        "KEEP_CLOSE_ONLY = False # debug to drop other columns\n",
        "USE_CPU = False\n",
        "FUTURE_DAYS= 5\n",
        "DROP_FIRST_N_DAYS=10000\n",
        "USE_TREND_PREDICTION = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and not USE_CPU else 'cpu')\n",
        "\n",
        "csv = './data/SPXDailyData.csv'\n",
        "df = GetDataset(csv)\n",
        "dataset = df.get_data2(FUTURE_DAYS) if USE_TREND_PREDICTION else df.get_data()\n",
        "dataset = dataset[:][DROP_FIRST_N_DAYS:]#discard ~first half\n",
        "if REPEAT_ONE_SMALL_BATCH:\n",
        "  dataset = dataset[0:10]\n",
        "  if FAKE_DATA:\n",
        "    dataset[:][0:5]=.6\n",
        "    dataset[:][5:]=.4\n",
        "    dataset['Next_Day_Change'][0:5]=0\n",
        "    dataset['Next_Day_Change'][5:]=1\n",
        "\n",
        "#split into 3\n",
        "valid_frac, test_frac = 0.2, 0.2\n",
        "train_sz=int(dataset.shape[0]*(1-(valid_frac+test_frac)))\n",
        "valid_sz=int(dataset.shape[0]*(valid_frac))\n",
        "df_train = dataset[               0:train_sz]\n",
        "df_valid = dataset[        train_sz:train_sz+valid_sz]\n",
        "df_test = dataset[train_sz+valid_sz:]\n",
        "\n",
        "if KEEP_CLOSE_ONLY:# see in case additional info acts like a noise\n",
        "  df_train.drop(columns=['Open', 'High', 'Low'], inplace=True)\n",
        "  df_valid.drop(columns=['Open', 'High', 'Low'], inplace=True)\n",
        "  df_test.drop(columns=['Open', 'High', 'Low'], inplace=True)\n",
        "\n",
        "#convert to sequence data and make dataset\n",
        "train_dataset = StockData(df_train.to_numpy(), num_days=NUM_DAYS)\n",
        "valid_dataset = StockData(df_valid.to_numpy(), num_days=NUM_DAYS) \n",
        "test_dataset = StockData(df_test.to_numpy(), num_days=NUM_DAYS) \n"
      ],
      "metadata": {
        "id": "rbH2c1ucncAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_dataset[:][0].shape)\n",
        "#print(train_dataset.num_samples)\n",
        "#print(dataset)\n",
        "#print(train_dataset[:])"
      ],
      "metadata": {
        "id": "DpBItwrOd6um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from models.my_transformer import TransformerModelImpl\n",
        "from torch.utils.data import DataLoader\n",
        "class hyperparameters:\n",
        "    device = device\n",
        "    n_layers = 1\n",
        "    num_heads = 1\n",
        "    model_dim = 4 #this is number of features\n",
        "    forward_dim = 64\n",
        "    output_dim = 1\n",
        "    dropout = 3e-4\n",
        "    n_epochs = 10\n",
        "    lr = 0.001 if USE_TRANSFORMER else 0.001\n",
        "    batch_size = 64\n",
        "modelT = TransformerModelImpl(hyperparameters).to(device)\n",
        "#summary(modelT, (NUM_DAYS, train_dataset[:][0].shape[2]))\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_dataset.num_samples if REPEAT_ONE_SMALL_BATCH else hyperparameters.batch_size, shuffle=False)#todo while debuggin set to false\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=valid_dataset.num_samples if REPEAT_ONE_SMALL_BATCH else hyperparameters.batch_size, shuffle=False)#todo while debuggin set to false"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aROsQr4KI7B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.my_transformer import TransformerModelImpl2\n",
        "class params:\n",
        "    device = device\n",
        "    num_layers = 1\n",
        "    nhead = 1\n",
        "    d_model = df_train.shape[1]-1 #this is number of features\n",
        "    dim_feedforward = 64\n",
        "    d_output = 1\n",
        "    dropout = 3e-4\n",
        "    seq_len=NUM_DAYS\n",
        "modelT2 = TransformerModelImpl2(params).to(device)\n",
        "#summary(modelT2, (NUM_DAYS, train_dataset[:][0].shape[2]))\n"
      ],
      "metadata": {
        "id": "cYE1baLwzKPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class FCNet(nn.Module):\n",
        "  def __init__(self,in_shape):\n",
        "    super(FCNet,self).__init__()\n",
        "    self.fc_layer1 = nn.Linear(in_shape,in_shape*4)\n",
        "    self.bn_layer1 = nn.BatchNorm1d(in_shape*4)\n",
        "    self.fc_layer2 = nn.Linear(in_shape*4,in_shape*8)\n",
        "    self.bn_layer2 = nn.BatchNorm1d(in_shape*8)\n",
        "    self.fc_layer3 = nn.Linear(in_shape*8,1)\n",
        "  def forward(self,x):\n",
        "    x = torch.flatten(x,start_dim=1)\n",
        "    x = self.bn_layer1(torch.relu(self.fc_layer1(x)))\n",
        "    x = self.bn_layer2(torch.relu(self.fc_layer2(x)))\n",
        "    x = self.fc_layer3(x)\n",
        "    return x.reshape(x.shape[0],-1)\n",
        "#print(train_dataset[:][0].shape[2])\n",
        "modelFC = FCNet((train_dataset[:][0].shape[2])*NUM_DAYS).to(device)\n",
        "#summary(modelFC, (NUM_DAYS, train_dataset[:][0].shape[2]))"
      ],
      "metadata": {
        "id": "9WfThZnHDyk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3q-ItwbSI7B5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from utils import plot_curves\n",
        "from utils import train\n",
        "from utils import evaluate\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "import copy\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "model = modelT2 if USE_TRANSFORMER and USE_ALT_TRANSFORMER else modelT if USE_TRANSFORMER else modelFC\n",
        "#minitial = copy.deepcopy(model)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters.lr)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=1)#constant\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "avg_train_loss,avg_train_acc,avg_valid_loss,avg_valid_acc=[],[],[],[]\n",
        "model.float()\n",
        "model.to(device)\n",
        "for epoch in range(hyperparameters.n_epochs):\n",
        "    train_loss, atl, ata = train(model, train_loader, optimizer, criterion, device)\n",
        "    #scheduler.step(train_loss)\n",
        "    _, avl, ava = evaluate(model, valid_loader, criterion, device)\n",
        "    if epoch%50==1:\n",
        "      print(\"Epoch %d: Training Loss: %.4f. Training Acc: %.4f. Validation Loss: %.4f. Validation Acc: %.4f.\" % (epoch+1, atl, ata, avl, ava))\n",
        "    avg_train_loss.append(atl.item())\n",
        "    avg_train_acc.append(ata)\n",
        "    avg_valid_loss.append(avl.item())\n",
        "    avg_valid_acc.append(ava)\n",
        "plot_curves(avg_train_loss,avg_train_acc,avg_valid_loss,avg_valid_acc, info='', save=False)   \n",
        "#mfinal = copy.deepcopy(model)\n",
        " "
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vBC0IgBjI7B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a,b=train_dataset[:]\n",
        "print(a.shape)\n",
        "print(b.shape)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "70Dz7C-d0UI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating Predictions"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wtrEo-8NI7B6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "model.eval()\n",
        "features, targets = test_dataset[:]\n",
        "features, targets = features.to(device), targets.to(device)\n",
        "predictions = model(features.float())\n",
        "pred=torch.round(torch.sigmoid(predictions)).long()\n",
        "print(classification_report(targets.cpu().detach().numpy(), pred.cpu().detach().numpy(), output_dict=True)['weighted avg']['f1-score'])\n",
        "print(classification_report(targets.cpu().detach().numpy(), pred.cpu().detach().numpy()))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ex-TS46DI7B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tcX2mjMMI7B8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1. Review/Fix Transformer Implementation\n",
        "# 2. Review/Fix Training and Testing\n",
        "# 3. Review/Fix Predictions with Tensors\n",
        "# 4. Implement Accuracy\n",
        "# 5. Implement Charting\n",
        "# 6. Experimentation"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V-sBVVEII7B8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "exp1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}