{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "exp3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install ray==1.12.0"
   ],
   "metadata": {
    "id": "WIh7LUlcmBwf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSoMmG02Xe-4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd '/content/drive/MyDrive/CS7643 Final Project/'  "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "import ray\n",
    "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total"
   ],
   "metadata": {
    "id": "gkAOivwwx1KA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from data.get_dataset import GetDataset, StockData\n",
    "\n",
    "FAKE_DATA=False\n",
    "REPEAT_ONE_SMALL_BATCH=False\n",
    "NUM_DAYS = 5\n",
    "FUTURE_DAYS= 5\n",
    "DROP_FIRST_N_DAYS=10000\n",
    "#USE_TRANSFORMER=False #ONLY DOES FULLY CONNECTED FOR NOW\n",
    "#USE_ALT_TRANSFORMER = False  #ONLY DOES FULLY CONNECTED FOR NOW\n",
    "NUM_SAMPLES=10 # FOR RAY TUNE\n",
    "MAX_NUM_EPOCHS=10\n",
    "CONFIG = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"batch_size\": tune.choice([32])\n",
    "    #\"batch_size\": tune.choice([2, 4]),\n",
    "    #\"dim_feedforward\": tune.sample_from(lambda _: 2 ** np.random.randint(4, 9)),#16 to 256\n",
    "    #\"num_layers\": tune.choice([1, 2, 3]),\n",
    "    #\"dropout\": tune.loguniform(1e-5, 5e-1)\n",
    "}\n",
    "\n",
    "csv = './data/SPXDailyData.csv'\n",
    "df = GetDataset(csv)\n",
    "dataset = df.get_data2(FUTURE_DAYS)\n",
    "dataset = dataset[:][DROP_FIRST_N_DAYS:]\n",
    "if REPEAT_ONE_SMALL_BATCH:\n",
    "  dataset = dataset[0:10]\n",
    "  if FAKE_DATA:\n",
    "    dataset[:][0:5]=.6\n",
    "    dataset[:][5:]=.4\n",
    "    dataset['Next_Day_Change'][0:5]=0\n",
    "    dataset['Next_Day_Change'][5:]=1\n",
    "\n",
    "#split into 3\n",
    "valid_frac, test_frac = 0.2, 0.2\n",
    "train_sz=int(dataset.shape[0]*(1-(valid_frac+test_frac)))\n",
    "valid_sz=int(dataset.shape[0]*(valid_frac))\n",
    "df_train = dataset[               0:train_sz]\n",
    "df_valid = dataset[        train_sz:train_sz+valid_sz]\n",
    "df_test = dataset[train_sz+valid_sz:]\n",
    "\n",
    "#convert to sequence data and make dataset\n",
    "train_dataset = StockData(df_train.to_numpy(), num_days=NUM_DAYS)\n",
    "valid_dataset = StockData(df_valid.to_numpy(), num_days=NUM_DAYS) \n",
    "test_dataset = StockData(df_test.to_numpy(), num_days=NUM_DAYS) \n"
   ],
   "metadata": {
    "id": "1aXmrCB2XtSQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FCNet(nn.Module):\n",
    "  def __init__(self,in_shape):\n",
    "    super(FCNet,self).__init__()\n",
    "    self.fc_layer1 = nn.Linear(in_shape,in_shape*4)\n",
    "    self.bn_layer1 = nn.BatchNorm1d(in_shape*4)\n",
    "    self.fc_layer2 = nn.Linear(in_shape*4,in_shape*8)\n",
    "    self.bn_layer2 = nn.BatchNorm1d(in_shape*8)\n",
    "    self.fc_layer3 = nn.Linear(in_shape*8,1)\n",
    "  def forward(self,x):\n",
    "    x = torch.flatten(x,start_dim=1)\n",
    "    x = self.bn_layer1(torch.relu(self.fc_layer1(x)))\n",
    "    x = self.bn_layer2(torch.relu(self.fc_layer2(x)))\n",
    "    x = self.fc_layer3(x)\n",
    "    return x.reshape(x.shape[0],-1)\n",
    "#class FCNet(nn.Module):\n",
    "#  def __init__(self,in_shape):\n",
    "#    super(FCNet,self).__init__()\n",
    "#    self.fc_layer1 = nn.Linear(in_shape,in_shape//2)\n",
    "#    self.bn_layer1 = nn.BatchNorm1d(in_shape//2)\n",
    "#    self.fc_layer2 = nn.Linear(in_shape//2,1)\n",
    "#  def forward(self,x):\n",
    "#    x = torch.flatten(x,start_dim=1)\n",
    "#    x = self.bn_layer1(torch.relu(self.fc_layer1(x)))\n",
    "#    x = self.fc_layer2(x)\n",
    "#    return x.reshape(x.shape[0],-1)\n",
    "\n",
    "#class FCNet(nn.Module):\n",
    "#  def __init__(self,in_shape):\n",
    "#    super(FCNet,self).__init__()\n",
    "#    self.fc_layer1 = nn.Linear(in_shape,1)\n",
    "#  def forward(self,x):\n",
    "#    x = torch.flatten(x,start_dim=1)\n",
    "#    x = self.fc_layer1(x)\n",
    "#    return x.reshape(x.shape[0],-1)    \n",
    "#print(train_dataset[:][0].shape[2])\n",
    "\n",
    "modelFC = FCNet((train_dataset[:][0].shape[2])*NUM_DAYS)"
   ],
   "metadata": {
    "id": "aIivxr91atNE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import plot_curves\n",
    "from utils import train\n",
    "from utils import evaluate\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import copy\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def train_self(config, checkpoint_dir=None, data_dir=None):\n",
    "   device = \"cpu\"\n",
    "   model = None\n",
    "   if torch.cuda.is_available():\n",
    "      device = \"cuda:0\"\n",
    "      model=modelFC\n",
    "      if torch.cuda.device_count() > 1:\n",
    "         model = nn.DataParallel(model)\n",
    "   else:\n",
    "      model=modelFC\n",
    "   model.to(device)\n",
    "\n",
    "   criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "   optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "   if checkpoint_dir:\n",
    "      model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "      model.load_state_dict(model_state)\n",
    "      optimizer.load_state_dict(optimizer_state)\n",
    "  \n",
    "   train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "   valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "    \n",
    "\n",
    "   avg_train_loss,avg_train_acc,avg_valid_loss,avg_valid_acc=[],[],[],[]\n",
    "   model.float()\n",
    "   for epoch in range(MAX_NUM_EPOCHS):\n",
    "      train_loss, atl, ata = train(model, train_loader, optimizer, criterion, device)\n",
    "      #scheduler.step(train_loss)\n",
    "      _, avl, ava = evaluate(model, valid_loader, criterion, device)\n",
    "      if epoch%50==1:\n",
    "         print(\"Epoch %d: Training Loss: %.4f. Training Acc: %.4f. Validation Loss: %.4f. Validation Acc: %.4f.\" % (epoch+1, atl, ata, avl, ava))\n",
    "      avg_train_loss.append(atl.item())\n",
    "      avg_train_acc.append(ata)\n",
    "      avg_valid_loss.append(avl.item())\n",
    "      avg_valid_acc.append(ava)\n",
    "\n",
    "      with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "      tune.report(loss=(avl.item()), accuracy=ava)\n",
    " "
   ],
   "metadata": {
    "id": "XYpvCRgjiH7_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#cite: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "GPUS_PER_TRIAL=2\n",
    "\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "\n",
    "config=CONFIG#see above\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=MAX_NUM_EPOCHS,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "reporter = JupyterNotebookReporter(\n",
    "    True, \n",
    "    # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "#result = tune.run(\n",
    "#    partial(train_self, data_dir=data_dir),\n",
    "#    resources_per_trial={\"cpu\": 2, \"gpu\": GPUS_PER_TRIAL},\n",
    "#    config=config,\n",
    "#    num_samples=NUM_SAMPLES,\n",
    "#    scheduler=scheduler,\n",
    "#    progress_reporter=reporter)\n",
    "\n",
    "result = tune.run(\n",
    "    train_self, \n",
    "    config=config, \n",
    "    verbose=1,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    progress_reporter=reporter# This is used to specify the logging directory.   \n",
    "    #stop={\"loss\": 0.3}  # This will stop the trial \n",
    ")\n",
    "\n",
    "print('=========================================')\n",
    "#print(result.results_df)\n",
    "#print(result.trial_dataframes)\n",
    "print('=========================================')\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "    best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "ax = None  # This plots everything on the same plot\n",
    "for d in result.trial_dataframes.values():\n",
    "    ax = d.loss.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "ax = None  # This plots everything on the same plot\n",
    "for d in result.trial_dataframes.values():\n",
    "    ax = d.accuracy.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_trained_model = None\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if GPUS_PER_TRIAL > 1:\n",
    "        best_trained_model = modelFC\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "else:\n",
    "   best_trained_model = modelFC\n",
    "best_trained_model.to(device)\n",
    "\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "\n",
    "#Todo later -> use eval\n",
    "#test_acc = test_accuracy(best_trained_model, device)\n",
    "#print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ],
   "metadata": {
    "id": "VVGymX7ZYHbm"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}